{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e00f613-f789-4e64-9b66-f2dc013920ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from vision_model import VisionModel, VisionProjector\n",
    "from text_model import LLaMA\n",
    "from types import SimpleNamespace\n",
    "from transformers import SiglipVisionModel, SiglipVisionConfig\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "471a4d1d-bd7e-47a9-ad2e-74a14d2b7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blinky(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        vision_config = SiglipVisionConfig.from_pretrained(self.config.vision_model_hf)\n",
    "        self.config.vision_config = SimpleNamespace(**vision_config.to_dict())\n",
    "        \n",
    "        self.vision = SiglipVisionModel(vision_config).to(dtype=self.config.dtype)\n",
    "        self.vision_proj = nn.Linear(self.config.vision_config.hidden_size * 4, self.config.embed_dim, bias=False, dtype=self.config.dtype)\n",
    "        self.text_model = LLaMA(self.config)\n",
    "\n",
    "    def pixel_shuffle(self, x, scale_factor=2):\n",
    "        bsz, seq, embed_dim = x.size()\n",
    "        height = width = int(seq**0.5)\n",
    "        x = x.view(bsz, height, width, embed_dim)\n",
    "        x = x.view(bsz, height, int(width / scale_factor), embed_dim * scale_factor)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(bsz, int(width / scale_factor), int(height / scale_factor), embed_dim * (scale_factor**2))\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(bsz, int(seq / (scale_factor**2)), embed_dim * (scale_factor**2))\n",
    "        return x\n",
    "\n",
    "    def prepare_for_training(self):\n",
    "        \n",
    "        from transformers import SiglipVisionModel, AutoModelForCausalLM\n",
    "        \n",
    "        vision = SiglipVisionModel.from_pretrained(self.config.vision_model_hf, torch_dtype=model.config.dtype)\n",
    "        self.vision.load_state_dict(vision.state_dict())\n",
    "\n",
    "        assert torch.allclose(\n",
    "            vision.vision_model.embeddings.position_embedding.weight, \n",
    "            self.vision.vision_model.embeddings.position_embedding.weight\n",
    "        ), 'couldnt load vision model'\n",
    "        \n",
    "        smol = AutoModelForCausalLM.from_pretrained(self.config.text_model_hf,torch_dtype=model.config.dtype)\n",
    "        smol_sd = smol.state_dict()\n",
    "        model_sd = self.text_model.state_dict()\n",
    "        smol_sd = {k:v for k,v in smol_sd.items() if not any([s in k for s in ['rope','causal_mask']])}\n",
    "        \n",
    "        for smol_key,smol_value in smol_sd.items():\n",
    "            model_key = smol_key.replace('model.','')\n",
    "            model_sd[model_key] = smol_value.clone()\n",
    "        \n",
    "        self.text_model.load_state_dict(model_sd)\n",
    "\n",
    "        assert torch.allclose(smol.lm_head.weight, self.text_model.lm_head.weight), 'couldnt load text model'\n",
    "    \n",
    "        del smol, vision\n",
    "        gc.collect()\n",
    "        \n",
    "    def forward_image_features(self, pixel_values):\n",
    "        x = self.vision(pixel_values).last_hidden_state\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.vision_proj(x)\n",
    "        return x\n",
    "\n",
    "    def _vision_trainable(self,trainable=False):\n",
    "        for p in self.vision.parameters():\n",
    "            p.requires_grad=trainable\n",
    "\n",
    "    def _text_trainable(self,trainable=False):\n",
    "        for n,p in self.text_model.named_parameters():\n",
    "            if 'embed_tokens' in n or 'lm_head' in n:\n",
    "                p.requires_grad = False\n",
    "            else:\n",
    "                p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, input_ids, pixel_values=None, attention_mask=None, labels=None):\n",
    "\n",
    "        x = self.text_model.embed_tokens(input_ids)\n",
    "\n",
    "        if pixel_values is not None:\n",
    "            image_tokens = self.forward_image_features(pixel_values)\n",
    "            x = torch.cat([image_tokens, x.detach()], dim=1)\n",
    "            attention_mask = torch.cat([\n",
    "                torch.full((x.shape[0],self.config.num_image_tokens),1).to(attention_mask.device).bool(), \n",
    "                attention_mask\n",
    "            ],dim=1)\n",
    "\n",
    "        for layer in self.text_model.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "            \n",
    "        x = self.text_model.norm(x)\n",
    "        logits = self.text_model.lm_head(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            return loss\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43c00b67-9ede-4dca-ba4e-7e00a2901667",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    embed_dim = 576,\n",
    "    intermediate_dim = 1536,\n",
    "    max_position_embeddings = 8192,\n",
    "    base_theta = 100000,\n",
    "    num_q_heads = 9,\n",
    "    num_kv_heads = 3,\n",
    "    attn_dropout = 0.,\n",
    "    num_layers = 30,\n",
    "    vocab_size = 49152,\n",
    "    eos_token_id = 2,\n",
    "    dtype = torch.bfloat16,\n",
    "    num_image_tokens = 256,\n",
    "    vision_model_hf = 'google/siglip2-base-patch16-512',\n",
    "    text_model_hf = 'HuggingFaceTB/SmolLM2-135M-Instruct'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98811d5b-bffa-4fe1-a032-3123bb38fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Blinky(config)\n",
    "model.prepare_for_training()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10c2cc5e-24d2-4d2e-9681-c955ccbe2ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "39a84340-88e0-4b9b-9f14-dd77f42920bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processor import BlinkyProcessor\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a5a8df00-f564-48b2-94d1-0f9532e0ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [{\n",
    "    'text': [{'role':'user','content':'hey!'}],\n",
    "    'image': Image.open('./tests/car.jpg')\n",
    "}]\n",
    "processor = BlinkyProcessor('./Blinky/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a6e2fcd9-2575-4cdb-b285-4ddc8943c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(sample)\n",
    "inputs['pixel_values'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de7506b6-5f9b-424d-9736-1427c3d22355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d926719-7f6e-4e50-8405-63311bfab634",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m max_tokens = \u001b[32m200\u001b[39m\n\u001b[32m      2\u001b[39m deterministic = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m context = \u001b[43minputs\u001b[49m[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].cuda()\n\u001b[32m      4\u001b[39m attention_mask = inputs[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].cuda() \n\u001b[32m      5\u001b[39m sequence = context\n",
      "\u001b[31mNameError\u001b[39m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "max_tokens = 200\n",
    "deterministic = True\n",
    "context = inputs['input_ids'].cuda()\n",
    "attention_mask = inputs['attention_mask'].cuda() \n",
    "sequence = context\n",
    "outputs=[]\n",
    "for _ in range(max_tokens):\n",
    "    with torch.inference_mode():\n",
    "        out = model(input_ids=sequence, attention_mask=attention_mask)\n",
    "    out = out[:,-1,:]\n",
    "    probs = F.softmax(out,dim=-1)\n",
    "    if deterministic:\n",
    "        next_token = torch.argmax(probs,dim=-1,keepdim=True)\n",
    "    else:\n",
    "        next_token = torch.multinomial(probs,num_samples=1)\n",
    "    outputs.append(processor.tokenizer.decode(next_token.flatten().cpu().numpy()))\n",
    "    sequence = torch.cat([sequence,next_token],dim=1)\n",
    "    attention_mask = torch.cat([attention_mask, torch.tensor([[True]]).cuda()],dim=1)\n",
    "    if next_token.item() == processor.tokenizer.eos_token_id:\n",
    "        break\n",
    "print(''.join(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e918f6-9c01-48d7-bd2c-55793c6cca01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
