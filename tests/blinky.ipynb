{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9ffd9a-2706-416e-b61f-857c84b0a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from vision_model import VisionModel, VisionProjector\n",
    "from text_model import LLaMA\n",
    "from types import SimpleNamespace\n",
    "from transformers import SiglipVisionModel, SiglipVisionConfig\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e556fd6e-d822-4833-bfd4-92d002235e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blinky(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        vision_config = SiglipVisionConfig.from_pretrained(self.config.vision_model_hf)\n",
    "        self.config.vision_config = SimpleNamespace(**vision_config.to_dict())\n",
    "        \n",
    "        self.vision = SiglipVisionModel(vision_config).to(dtype=self.config.dtype)\n",
    "        self.vision_proj = nn.Linear(self.config.vision_config.hidden_size * 4, self.config.embed_dim, bias=False, dtype=self.config.dtype)\n",
    "        self.text_model = LLaMA(self.config)\n",
    "\n",
    "    def pixel_shuffle(self, x, scale_factor=2):\n",
    "        bsz, seq, embed_dim = x.size()\n",
    "        height = width = int(seq**0.5)\n",
    "        x = x.view(bsz, height, width, embed_dim)\n",
    "        x = x.view(bsz, height, int(width / scale_factor), embed_dim * scale_factor)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(bsz, int(width / scale_factor), int(height / scale_factor), embed_dim * (scale_factor**2))\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(bsz, int(seq / (scale_factor**2)), embed_dim * (scale_factor**2))\n",
    "        return x\n",
    "\n",
    "    def prepare_for_training(self):\n",
    "        \n",
    "        from transformers import SiglipVisionModel, AutoModelForCausalLM\n",
    "        \n",
    "        vision = SiglipVisionModel.from_pretrained(self.config.vision_model_hf, torch_dtype=model.config.dtype)\n",
    "        self.vision.load_state_dict(vision.state_dict())\n",
    "\n",
    "        assert torch.allclose(\n",
    "            vision.vision_model.embeddings.position_embedding.weight, \n",
    "            self.vision.vision_model.embeddings.position_embedding.weight\n",
    "        ), 'couldnt load vision model'\n",
    "        \n",
    "        smol = AutoModelForCausalLM.from_pretrained(self.config.text_model_hf,torch_dtype=model.config.dtype)\n",
    "        smol_sd = smol.state_dict()\n",
    "        model_sd = self.text_model.state_dict()\n",
    "        smol_sd = {k:v for k,v in smol_sd.items() if not any([s in k for s in ['rope','causal_mask']])}\n",
    "        \n",
    "        for smol_key,smol_value in smol_sd.items():\n",
    "            model_key = smol_key.replace('model.','')\n",
    "            model_sd[model_key] = smol_value.clone()\n",
    "        \n",
    "        self.text_model.load_state_dict(model_sd)\n",
    "\n",
    "        assert torch.allclose(smol.lm_head.weight, self.text_model.lm_head.weight), 'couldnt load text model'\n",
    "    \n",
    "        del smol, vision\n",
    "        gc.collect()\n",
    "        \n",
    "    def forward_image_features(self, pixel_values):\n",
    "        x = self.vision(pixel_values).last_hidden_state\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.vision_proj(x)\n",
    "        return x\n",
    "\n",
    "    def _vision_trainable(self,trainable=False):\n",
    "        for p in self.vision.parameters():\n",
    "            p.requires_grad=trainable\n",
    "\n",
    "    def _text_trainable(self,trainable=False):\n",
    "        for n,p in self.text_model.named_parameters():\n",
    "            if 'embed_tokens' in n or 'lm_head' in n:\n",
    "                p.requires_grad = False\n",
    "            else:\n",
    "                p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, input_ids, pixel_values=None, attention_mask=None, labels=None):\n",
    "\n",
    "        x = self.text_model.embed_tokens(input_ids)\n",
    "\n",
    "        if pixel_values is not None:\n",
    "            image_tokens = self.forward_image_features(pixel_values)\n",
    "            x = torch.cat([image_tokens, x.detach()], dim=1)\n",
    "            attention_mask = torch.cat([\n",
    "                torch.full((x.shape[0],self.config.num_image_tokens),1).to(attention_mask.device).bool(), \n",
    "                attention_mask\n",
    "            ],dim=1)\n",
    "\n",
    "        for layer in self.text_model.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "            \n",
    "        x = self.text_model.norm(x)\n",
    "        logits = self.text_model.lm_head(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            return loss\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e30b09-cc15-4d8b-b957-311940b5eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = SimpleNamespace(\n",
    "#     embed_dim = 576,\n",
    "#     intermediate_dim = 1536,\n",
    "#     max_position_embeddings = 8192,\n",
    "#     base_theta = 100000,\n",
    "#     num_q_heads = 9,\n",
    "#     num_kv_heads = 3,\n",
    "#     attn_dropout = 0.,\n",
    "#     num_layers = 30,\n",
    "#     vocab_size = 49152,\n",
    "#     eos_token_id = 2,\n",
    "#     dtype = torch.bfloat16,\n",
    "#     num_image_tokens = 256,\n",
    "#     vision_model_hf = 'google/siglip2-base-patch16-512',\n",
    "#     text_model_hf = 'HuggingFaceTB/SmolLM2-135M-Instruct'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59aa04f2-2519-41b1-9a14-79103dd12fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Blinky(config)\n",
    "# model.prepare_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28698de0-75a3-477a-8e19-22de4a97b274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da901f9-0e44-4a3a-bbdc-be53100b71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = {\n",
    "#     'input_ids': torch.randint(0,config.vocab_size,(1,120)),\n",
    "#     'pixel_values': torch.rand(1,3,512,512),\n",
    "#     'attention_mask': torch.ones(1,120).bool()\n",
    "#     # 'labels': torch.randint(0,config.vocab_size,(1,120+256))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0c1591-dbef-47e1-8fb0-ecf6eae87dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a744610e-a30c-4a85-94fa-d9251328fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0775328b-4a64-4fa0-a364-2ef3980053ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessor import BlinkyProcessor\n",
    "# from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8992978b-9928-4c25-ba5a-970afe1ae383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = [{\n",
    "#     'text': [{'role':'user','content':'hey!'}],\n",
    "#     'image': Image.open('./tests/car.jpg')\n",
    "# },\n",
    "#           {\n",
    "#     'text': [{'role':'user','content':'this is a test of padding :)'}],\n",
    "#     'image': Image.open('./tests/car.jpg')\n",
    "# }\n",
    "#          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaa7ae91-6ba6-4936-ba4d-a0ff213c995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "134d4f4b-292b-4887-9a0d-9c6e2a9d89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = BlinkyProcessor('./Blinky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9ae3d39-2cd1-42ae-8517-0f4888c25c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = processor(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caab45c9-84a5-47ef-95d8-3c044dc4c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38d6c267-4bea-4ff9-a408-c47effbba6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9accd0a-03dc-456a-9852-ce0b1d8cf0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['attention_mask'].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d80694cd-789e-4f99-9241-ab234deedbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(processor.tokenizer.decode(inputs['input_ids'].flatten().numpy(),skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3f98591-e075-4682-a837-def1ecf3dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e150f2f-8422-4a2e-9e53-48e7cc2241eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([[1,2,3,4,5,3],[2,3,4,5,3,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9a3c0d8-cfb4-4d0b-97f9-8e863a2a9a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1, 1]), tensor([2, 5, 1, 4]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(x==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fde87629-c298-4af7-befe-6dda36c1c52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.where(x==3, torch.arange(x.size(1)), torch.tensor(-1))\n",
    "last_indices = indices.max(dim=1).values + 1\n",
    "last_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2480197-ac9d-4ae3-80fc-8375acd44622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor([\n",
    "    [1,1,1,1,1,1,0],\n",
    "    [1,1,1,1,0,0,0],\n",
    "    [1,1,1,1,1,0,0]\n",
    "])\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18009e6a-f9a4-4b9e-aa76-3cd8b5f78c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = torch.triu(torch.ones(7,7),diagonal=1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "409e747e-bce1-4151-aca9-b2ec30ea2891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m[:,None,None,:].bool() * ~cm[None,None,:,:].bool()).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "243408e6-4e9e-43c3-959b-1232bf5ef4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 7, 7])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m[:,None,None,:] * cm[None,None,:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fad4d4f-4c09-4c23-b6f3-59e5c2482efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4816, 0.5184, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4037, 0.2149, 0.3814, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2203, 0.1876, 0.3229, 0.2692, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2314, 0.1145, 0.2406, 0.2657, 0.1478, 0.0000, 0.0000],\n",
       "        [0.2353, 0.1512, 0.1142, 0.2233, 0.1591, 0.1170, 0.0000],\n",
       "        [0.1160, 0.1019, 0.1011, 0.1441, 0.1470, 0.2090, 0.1809]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(7,7).masked_fill_(cm.bool(),-torch.inf).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77923da7-17fd-4bcc-9f76-3798a14a8693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0166,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "          [0.5711, 0.5664,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "          [0.3196, 0.1051, 0.1221,   -inf,   -inf,   -inf,   -inf],\n",
       "          [0.4551, 0.7997, 0.9236, 0.9577,   -inf,   -inf,   -inf],\n",
       "          [0.5837, 0.9141, 0.3334, 0.4196, 0.1097,   -inf,   -inf],\n",
       "          [0.8766, 0.3978, 0.7507, 0.4006, 0.5501, 0.2624,   -inf],\n",
       "          [0.8284, 0.3576, 0.8333, 0.3182, 0.0516, 0.1359,   -inf]]],\n",
       "\n",
       "\n",
       "        [[[0.0381,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "          [0.2121, 0.0872,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "          [0.1453, 0.0879, 0.7282,   -inf,   -inf,   -inf,   -inf],\n",
       "          [0.7898, 0.2131, 0.2304, 0.4859,   -inf,   -inf,   -inf],\n",
       "          [0.2660, 0.8900, 0.2929, 0.3275,   -inf,   -inf,   -inf],\n",
       "          [0.2877, 0.8541, 0.4545, 0.8825,   -inf,   -inf,   -inf],\n",
       "          [0.0364, 0.0222, 0.0946, 0.9770,   -inf,   -inf,   -inf]]],\n",
       "\n",
       "\n",
       "        [[[0.4313,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "          [0.0865, 0.9845,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "          [0.8426, 0.5303, 0.1792,   -inf,   -inf,   -inf,   -inf],\n",
       "          [0.5895, 0.7140, 0.7557, 0.7739,   -inf,   -inf,   -inf],\n",
       "          [0.6983, 0.6590, 0.6863, 0.9825, 0.4544,   -inf,   -inf],\n",
       "          [0.8283, 0.0233, 0.7878, 0.5583, 0.6044,   -inf,   -inf],\n",
       "          [0.6950, 0.6156, 0.2763, 0.0911, 0.8829,   -inf,   -inf]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,1,7,7).masked_fill_(~m[:,None,None,:].bool(),-torch.inf).masked_fill_(cm.bool(),-torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706f429-a6d8-4761-9ee4-0bc2083d77ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2ddb489-58c9-4f63-ae15-458710d06542",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=(torch.rand(3,1,7,7).masked_fill_(~(m[:,None,None,:].bool() * ~cm.bool()), -torch.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e07f90b-a1ad-416d-8084-8d80c631df83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6553, 0.3447, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3891, 0.2175, 0.3934, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2213, 0.2971, 0.2469, 0.2347, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2705, 0.1384, 0.1345, 0.3305, 0.1261, 0.0000, 0.0000],\n",
       "          [0.2313, 0.1304, 0.1350, 0.1765, 0.1145, 0.2123, 0.0000],\n",
       "          [0.1276, 0.1300, 0.1815, 0.2717, 0.1853, 0.1040, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6273, 0.3727, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3385, 0.2492, 0.4124, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2822, 0.2526, 0.2771, 0.1881, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2707, 0.1890, 0.3360, 0.2043, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3183, 0.1588, 0.2345, 0.2884, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1873, 0.2952, 0.2842, 0.2333, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4193, 0.5807, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2729, 0.4681, 0.2590, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3057, 0.1819, 0.3379, 0.1745, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2241, 0.2179, 0.2126, 0.1893, 0.1561, 0.0000, 0.0000],\n",
       "          [0.1193, 0.2863, 0.1588, 0.1554, 0.2801, 0.0000, 0.0000],\n",
       "          [0.2129, 0.3313, 0.1399, 0.1599, 0.1560, 0.0000, 0.0000]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd6887a8-18bf-4162-a8a8-8ca0fc9eb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask1(cm, pm):\n",
    "    return ~(pm[:,None,None,:].bool() * ~cm.bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2d2c379-c2fa-4021-aaee-b57fec802ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1]]],\n",
       "\n",
       "\n",
       "        [[[0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_attention_mask1(cm, m).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5f30d39-e638-4953-b23f-b7ae6ae8c3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 7])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6623801f-80af-4027-8050-9a058d47e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prefix_mask(seq_len,prefix_len):\n",
    "    causal_mask = torch.triu(torch.ones(seq_len,seq_len),diagonal=1)\n",
    "    causal_mask[:prefix_len,:prefix_len] = 0\n",
    "    return causal_mask\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    causal_mask = torch.triu(torch.ones(seq_len,seq_len),diagonal=1)\n",
    "    return causal_mask\n",
    "\n",
    "def create_attention_mask(causal_mask, pad_mask):\n",
    "    return ~(pad_mask[:,None,None,:].bool() * ~causal_mask.bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79d9c31c-52bd-4460-a6e8-722165eac8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask = torch.tensor([\n",
    "    [1,1,1,1,1,1,0],\n",
    "    [1,1,1,1,0,0,0],\n",
    "    [1,1,1,1,1,0,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dc5a0f8-a909-45bf-af62-712d0a8ac87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_mask = create_prefix_mask(7,3)\n",
    "prefix_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f56aa6a0-023f-4283-9316-1351a6238b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask = create_causal_mask(7)\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0d1ff08-d94c-4faf-9339-a456fffa2263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1]]],\n",
       "\n",
       "\n",
       "        [[[0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask = create_attention_mask(causal_mask, pad_mask)\n",
    "attn_mask.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adcd3c92-bd83-4a90-9ebc-d7cb5568d141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 7, 7])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7172feb8-fa1f-4ca2-b1f5-53a9f08955d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1]]],\n",
       "\n",
       "\n",
       "        [[[0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = attn_mask.clone()\n",
    "prefix[:,:,:3,:3] = 0\n",
    "prefix.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eca3fa2b-5ea1-4399-ab6a-c6f1c69a251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2786, 0.2820, 0.4394, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4246, 0.2865, 0.2889, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4656, 0.2683, 0.2661, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3674, 0.1457, 0.3229, 0.1640, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2532, 0.1962, 0.1129, 0.1455, 0.2921, 0.0000, 0.0000],\n",
       "          [0.1102, 0.1965, 0.2554, 0.1140, 0.1391, 0.1848, 0.0000],\n",
       "          [0.1808, 0.1188, 0.1249, 0.1380, 0.2904, 0.1471, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.2236, 0.5352, 0.2412, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3213, 0.3471, 0.3316, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4099, 0.2979, 0.2922, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1732, 0.2284, 0.3782, 0.2202, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1754, 0.2954, 0.2953, 0.2340, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2778, 0.3132, 0.1797, 0.2293, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2764, 0.3114, 0.1995, 0.2127, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.3089, 0.3921, 0.2991, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3271, 0.3322, 0.3407, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2325, 0.2895, 0.4779, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2311, 0.2805, 0.2827, 0.2057, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1832, 0.2989, 0.1532, 0.1809, 0.1838, 0.0000, 0.0000],\n",
       "          [0.1815, 0.2140, 0.2269, 0.1149, 0.2627, 0.0000, 0.0000],\n",
       "          [0.2403, 0.1641, 0.1631, 0.1567, 0.2758, 0.0000, 0.0000]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,1,7,7).masked_fill_(prefix,-torch.inf).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0661e5-0307-4917-b00f-69068171b340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
